{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate statistics by merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "from rpy2.rinterface import FloatSexpVector\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "pandas2ri.activate()\n",
    "base = importr('base')\n",
    "utils = importr('utils')\n",
    "\n",
    "utils.chooseCRANmirror(ind=1) # select the first mirror in the list\n",
    "packnames = ['orddom', 'dplyr', 'nortest', 'effsize']\n",
    "\n",
    "# Selectively install what needs to be install.\n",
    "# We are fancy, just because we can.\n",
    "names_to_install = [x for x in packnames if not rpackages.isinstalled(x)]\n",
    "if len(names_to_install) > 0:\n",
    "    utils.install_packages(StrVector(names_to_install))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orddom = importr('orddom')\n",
    "dplyr = importr('dplyr')\n",
    "rstats = importr('stats')\n",
    "nortest = importr('nortest')\n",
    "effsize = importr('effsize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182273"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"../Dataset-SBES2020.xlsx\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17819, 164454)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_conflicts = df[df[\"Conflicts\"] == \"YES\"]\n",
    "without_conflicts = df[df[\"Conflicts\"] != \"YES\"]\n",
    "len(with_conflicts), len(without_conflicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.775995347637883"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(with_conflicts) / len(df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpaired_t_test(rx, ry, x, y):\n",
    "    result = rstats.t_test(rx, ry, paired=False)\n",
    "    return result[result.names.index('statistic')][0], result[result.names.index('p.value')][0]\n",
    "\n",
    "def mann_whitney(rx, ry, x, y):\n",
    "    result = rstats.wilcox_test(rx, ry, alternative=\"two.sided\")\n",
    "    return result[result.names.index('statistic')][0], result[result.names.index('p.value')][0]\n",
    "\n",
    "def cohend(rx, ry, x, y):\n",
    "    e = effsize.cohen_d(rx, ry, paired=False)\n",
    "    return e[e.names.index('estimate')][0], e[e.names.index('magnitude')].levels[0]\n",
    "\n",
    "def cliffsdelta(rx, ry, x, y):\n",
    "    e = effsize.cliff_delta(rx, ry, paired=False)\n",
    "    return e[e.names.index('estimate')][0], e[e.names.index('magnitude')].levels[0]\n",
    "\n",
    "\n",
    "\n",
    "P = 0.05\n",
    "UNPAIRED_TESTS = {\n",
    "    True: (\"Unpaired T test\", unpaired_t_test, 'Cohen\\'s D', cohend),\n",
    "    False: (\"Mann-Whitney\", mann_whitney, 'Cliff\\'s Delta', cliffsdelta),\n",
    "}\n",
    "\n",
    "\n",
    "HEADER = [\"Project\", \"Attribute\", \"W/ C Mean\", \"W/O C Mean\",\n",
    " \"W/ C Kurtosis\", \"W/O C Kurtosis\", \"W/ C Normal Kurtosis\", \"W/O C Normal Kurtosis\",\n",
    " \"W/ C Anderson-Darling\", \"W/O C Anderson-Darling\", \"W/ C Normal Anderson-Darling\", \"W/O C Normal Anderson-Darling\",\n",
    " \"Inconsistencies\",\n",
    " \"Test\", \"P-value\",\n",
    " \"Effect Size\", \"Delta\", \"Meaning\"\n",
    "]\n",
    "\n",
    "def calculate_row(project, attr, with_conflicts, without_conflicts, P=P, TESTS=UNPAIRED_TESTS):\n",
    "    row = [project, attr]\n",
    "    wc_attr = with_conflicts[attr]\n",
    "    wo_attr = without_conflicts[attr]\n",
    "    row.append(wc_attr.mean())\n",
    "    row.append(wo_attr.mean())\n",
    "    \n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "         rwc_attr = ro.conversion.py2ri(wc_attr)\n",
    "         rwo_attr = ro.conversion.py2ri(wo_attr)\n",
    "\n",
    "    row.append(stats.normaltest(wc_attr).pvalue) # With Conflicts Kurtosis\n",
    "    row.append(stats.normaltest(wo_attr).pvalue) # Without Conflicts Kurtosis\n",
    "    row.append(row[-2] >= P)\n",
    "    row.append(row[-1] >= P)\n",
    "    row.append(pandas2ri.ri2py_floatvector(nortest.ad_test(rwc_attr)[1])[0]) # With Conflicts Anderson-Darling\n",
    "    row.append(pandas2ri.ri2py_floatvector(nortest.ad_test(rwo_attr)[1])[0]) # Without Conflicts Anderson-Darling\n",
    "    row.append(row[-2] >= P)\n",
    "    row.append(row[-1] >= P)\n",
    "    row.append(row[-2] != row[-6] or row[-1] != row[-5])\n",
    "    test_name, test, effect_name, effect = TESTS[row[-5] and row[-6]]\n",
    "    s, pvalue = test(rwo_attr, rwc_attr, wo_attr, wc_attr)\n",
    "    row.append(test_name)\n",
    "    row.append(pvalue)\n",
    "    if pvalue < P:\n",
    "        estimate, meaning = effect(rwo_attr, rwc_attr, wo_attr, wc_attr)\n",
    "        row.append(effect_name)\n",
    "        row.append(estimate)\n",
    "        row.append(meaning)\n",
    "    else:\n",
    "        row.append(\"No\")\n",
    "        row.append('-')\n",
    "        row.append('-')\n",
    "    return row\n",
    "    \n",
    "def calculate_attributes(result, project, attributes, with_conflicts, without_conflicts, P=P, TESTS=UNPAIRED_TESTS):\n",
    "    for attr in attributes: \n",
    "        result.append(calculate_row(project, attr, with_conflicts, without_conflicts, P=P, TESTS=TESTS))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"Branching-duration\", \"Total-duration\", \"Commits B1\", \"Commits B2\", \"Committers B1\", \"Committers B2\", \"Changed Files B1\", \"Changed Files B2\"]\n",
    "\n",
    "result = []\n",
    "calculate_attributes(result, \"<all>\", attributes, with_conflicts, without_conflicts, P=P, TESTS=UNPAIRED_TESTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joao\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1450: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "648"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "calculate_attributes(result, \"<all>\", attributes, with_conflicts, without_conflicts, P=P, TESTS=UNPAIRED_TESTS)\n",
    "for project in df[\"Project\"].unique():\n",
    "    p_wc = with_conflicts[with_conflicts[\"Project\"] == project]\n",
    "    p_wo = without_conflicts[without_conflicts[\"Project\"] == project]\n",
    "    calculate_attributes(result, project, attributes, p_wc, p_wo, P=P, TESTS=UNPAIRED_TESTS)\n",
    "    \n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(result, columns=HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qbittorrent 10 1310\n"
     ]
    }
   ],
   "source": [
    "for project in df[\"Project\"].unique():\n",
    "    p_wc = with_conflicts[with_conflicts[\"Project\"] == project]\n",
    "    p_wo = without_conflicts[without_conflicts[\"Project\"] == project]\n",
    "    if len(p_wc) < 20 or len(p_wo) < 20:\n",
    "        print(project, len(p_wc), len(p_wo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(df[\"Project\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook created the `statistics.csv` file"
   ]
  }
 ],
 "metadata": {
  "julynter-check-restart": true,
  "julynter-results": {
   "filteredId": [],
   "filteredIndividual": [],
   "filteredRestart": [],
   "filteredType": [],
   "hash": "291414d9309afa44d021583c8d92d50449eb2637",
   "visible": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
